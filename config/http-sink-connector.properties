# ==================================================
# HTTP Sink Connector Configuration Example
# ==================================================

# Connector Configuration
name=http-sink-connector
connector.class=com.devinblack.kafka.connect.http.HttpSinkConnector
tasks.max=3
topics=events

# ==================================================
# HTTP ENDPOINT CONFIGURATION
# ==================================================

# HTTP endpoint URL (required)
http.api.url=https://api.example.com/events

# HTTP method: POST, PUT, or DELETE
http.method=POST

# Timeouts in milliseconds
http.request.timeout.ms=30000
http.connection.timeout.ms=5000

# Connection pooling
http.max.connections.per.route=20
http.max.connections.total=100

# ==================================================
# AUTHENTICATION CONFIGURATION
# ==================================================

# Authentication type: none, basic, bearer, apikey, oauth2
auth.type=none

# -----------------------------
# Basic Authentication (uncomment if auth.type=basic)
# -----------------------------
#auth.basic.username=myuser
#auth.basic.password=${file:/path/to/secrets.properties:password}

# -----------------------------
# Bearer Token (uncomment if auth.type=bearer)
# -----------------------------
#auth.bearer.token=${file:/path/to/secrets.properties:bearer_token}

# -----------------------------
# API Key (uncomment if auth.type=apikey)
# -----------------------------
#auth.apikey.name=X-API-Key
#auth.apikey.value=${file:/path/to/secrets.properties:api_key}
#auth.apikey.location=header

# -----------------------------
# OAuth2 Client Credentials (uncomment if auth.type=oauth2)
# -----------------------------
#auth.oauth2.token.url=https://auth.example.com/oauth/token
#auth.oauth2.client.id=your-client-id
#auth.oauth2.client.secret=${file:/path/to/secrets.properties:oauth_secret}
#auth.oauth2.scope=read write
#auth.oauth2.token.expiry.buffer.seconds=300

# ==================================================
# HEADER FORWARDING CONFIGURATION
# ==================================================

# Enable forwarding Kafka headers to HTTP request
headers.forward.enabled=true

# Comma-separated list of headers to include (empty = all headers)
# Leave empty to forward all headers
headers.forward.include=

# Comma-separated list of headers to exclude
headers.forward.exclude=

# Prefix to add to all forwarded headers
# Example: if prefix is "X-Kafka-", header "correlation-id" becomes "X-Kafka-correlation-id"
headers.forward.prefix=

# Static headers to add to all requests
# Format: key1:value1,key2:value2
headers.static=X-Source:kafka,X-Application:http-sink-connector

# ==================================================
# RESPONSE HANDLING CONFIGURATION
# ==================================================

# Enable sending HTTP responses back to a Kafka topic
response.topic.enabled=false

# Response topic name (supports ${topic} variable for dynamic naming)
# Example: ${topic}-responses will create a response topic for each source topic
#response.topic.name=${topic}-responses

# Include original record key in response record
response.include.original.key=true

# Include original record headers in response record (prefixed with "original-")
response.include.original.headers=true

# Include request metadata in response headers
# Adds headers: http-status-code, http-response-time-ms, original-topic, original-partition, original-offset
response.include.request.metadata=true

# ==================================================
# ERROR HANDLING CONFIGURATION
# ==================================================

# Behavior when encountering null values: fail or ignore
behavior.on.null.values=fail

# Behavior on HTTP errors: fail or log
# fail = stop connector on error
# log = log error and continue
behavior.on.error=fail

# Error tolerance: none or all
# none = fail on first error
# all = continue on errors and send to DLQ
errors.tolerance=none

# Dead letter queue topic name (required if errors.tolerance=all)
#errors.deadletterqueue.topic.name=http-sink-dlq

# ==================================================
# RETRY CONFIGURATION
# ==================================================

# Enable retry on failure
retry.enabled=true

# Maximum number of retry attempts
retry.max.attempts=5

# Initial backoff in milliseconds (will be multiplied by backoff.multiplier on each retry)
retry.backoff.initial.ms=1000

# Maximum backoff in milliseconds
retry.backoff.max.ms=60000

# Backoff multiplier (exponential backoff)
retry.backoff.multiplier=2.0

# HTTP status codes to retry on (comma-separated)
# 429 = Too Many Requests
# 500 = Internal Server Error
# 502 = Bad Gateway
# 503 = Service Unavailable
# 504 = Gateway Timeout
retry.on.status.codes=429,500,502,503,504

# ==================================================
# CONVERTER CONFIGURATION (Optional)
# ==================================================

# These are typically configured globally in worker.properties
# Uncomment to override for this connector

#value.converter=org.apache.kafka.connect.json.JsonConverter
#value.converter.schemas.enable=false
#key.converter=org.apache.kafka.connect.storage.StringConverter
